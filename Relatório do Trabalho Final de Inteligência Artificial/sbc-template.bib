@article{russell2003artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  year={2003},
  publisher={Pearson Education Limited}
}

@article{haugeland1989artificial,
  title={Artificial intelligence: The very idea},
  author={Haugeland, John},
  journal={MIT press},
  volume={10},
  number={1},
  pages={1--35},
  year={1989}
}

@book{haykin2009neural,
  title={Neural networks and learning machines},
  author={Haykin, Simon S},
  year={2009},
  publisher={Pearson Education}
}

@article{kohonen2012self,
  title={Self-organizing maps},
  author={Kohonen, Teuvo},
  journal={Springer Series in Information Sciences},
  volume={30},
  pages={1--683},
  year={2012}
}

@inproceedings{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  booktitle={Science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006}
}

@inproceedings{bengio2009learning,
  title={Learning deep architectures for AI},
  author={Bengio, Yoshua},
  booktitle={Foundations and trends in Machine Learning},
  volume={2},
  number={1},
  pages={1--127},
  year={2009},
  publisher={Now Publishers Inc.}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{lipton2018mythos,
  title={The mythos of model interpretability},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018}
}

@inproceedings{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  booktitle={Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1319--1328},
  year={2017}
}

@inproceedings{besold2017neural,
  title={Neural-symbolic learning and reasoning: A survey and interpretation},
  author={Besold, Tarek R and Garcez, Artur d'Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Küchler, Kai-Uwe and Lamb, Luís C and Lowd, Daniel and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{garcez2002neural,
  title={Neural-symbolic learning systems: foundations and applications},
  author={Garcez, Artur S d'Avila and Broda, Krysia and Gabbay, Dov M},
  booktitle={Proceedings of the 17th International Joint Conference on Artificial Intelligence-Volume 1},
  pages={488--493},
  year={2002}
}

@article{bottou2017geometric,
  title={Geometric deep learning: going beyond euclidean data},
  author={Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={18--42},
  year={2017},
  publisher={IEEE}
}

@inproceedings{hu2018harnessing,
  title={Harnessing deep neural networks with logic rules},
  author={Hu, Zhiting and Ma, Xuezhe and Liu, Jian-Guang and Hovy, Eduard and Xing, Eric},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2410--2420},
  year={2018}
}

@article{andrews1995survey,
  title={Survey and critique of techniques for extracting rules from trained artificial neural networks},
  author={Andrews, Robert and Diederich, Joachim and Tickle, Alan B},
  journal={Knowledge-based systems},
  volume={8},
  number={6},
  pages={373--389},
  year={1995},
  publisher={Elsevier}
}

@article{serafini2016logic,
  title={Logic tensor networks: Deep learning and logical reasoning from data and knowledge},
  author={Serafini, Luciano and Garcez, Artur d’Avila},
  journal={arXiv preprint arXiv:1606.04422},
  year={2016}
}

@inproceedings{garcez2007connectionist,
  title={Connectionist modal logic: Representing modalities in neural networks},
  author={Garcez, Artur S d'Avila and Lamb, Luís C and Gabbay, Dov M},
  booktitle={Theoretical Computer Science},
  volume={371},
  number={1-2},
  pages={34--53},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{badreddine2020reinforcement,
  title={Reinforcement learning with pretrained neuralsymbolic representations for zero-shot generalization},
  author={Badreddine, Amine El and Donnat, Claire and Lou, Jean-Michel and Medioni, Gilles},
  booktitle={Computer Vision and Pattern Recognition Workshops (CVPRW)},
  pages={1671--1680},
  year={2020},
  organization={IEEE}
}

@article{adadi2018peeking,
  title={Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)},
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE access},
  volume={6},
  pages={52138--52160},
  year={2018},
  publisher={IEEE}
}

@article{DAVILAGARCEZ2001155,
title = {Symbolic knowledge extraction from trained neural networks: A sound approach},
journal = {Artificial Intelligence},
volume = {125},
number = {1},
pages = {155-207},
year = {2001},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(00)00077-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370200000771},
author = {A.S {d'Avila Garcez} and K Broda and D.M Gabbay},
keywords = {Neural-symbolic integration, Rule extraction, Nonmonotonic reasoning, Artificial neural networks},
abstract = {Although neural networks have shown very good performance in many application domains, one of their main drawbacks lies in the incapacity to provide an explanation for the underlying reasoning mechanisms. The “explanation capability” of neural networks can be achieved by the extraction of symbolic knowledge. In this paper, we present a new method of extraction that captures nonmonotonic rules encoded in the network, and prove that such a method is sound. We start by discussing some of the main problems of knowledge extraction methods. We then discuss how these problems may be ameliorated. To this end, a partial ordering on the set of input vectors of a network is defined, as well as a number of pruning and simplification rules. The pruning rules are then used to reduce the search space of the extraction algorithm during a pedagogical extraction, whereas the simplification rules are used to reduce the size of the extracted set of rules. We show that, in the case of regular networks, the extraction algorithm is sound and complete. We proceed to extend the extraction algorithm to the class of non-regular networks, the general case. We show that non-regular networks always contain regularities in their subnetworks. As a result, the underlying extraction method for regular networks can be applied, but now in a decompositional fashion. In order to combine the sets of rules extracted from each subnetwork into the final set of rules, we use a method whereby we are able to keep the soundness of the extraction algorithm. Finally, we present the results of an empirical analysis of the extraction system, using traditional examples and real-world application problems. The results have shown that a very high fidelity between the extracted set of rules and the network can be achieved.}
}

@article{smolensky1988connectionist,
  title={On the proper treatment of connectionism},
  author={Smolensky, Paul},
  journal={Behavioral and brain sciences},
  volume={11},
  number={1},
  pages={1--74},
  year={1988},
  publisher={Cambridge University Press}
}

@article{manha2014neural,
  title={Neural-symbolic cognitive reasoning: A step into first-order logic},
  author={Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and De Raedt, Luc and Demeester, Hendrik},
  journal={arXiv preprint arXiv:1406.1038},
  year={2014}
}

@article{dalle2021glm,
  title={GLM: General language model pretraining with autoregressive blank infilling},
  author={Du, Zihang and Huang, Shuming and Zheng, Wenting and Dong, Yu and Wang, Pengcheng and He, Guolin and Zhou, Wei and Zhao, Zhilin and Xiang, Tianyu and Xie, Zewen and others},
  journal={arXiv preprint arXiv:2112.01074},
  year={2021}
}

@article{rudin2019stop,
  title={Stop explaining black box models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature},
  volume={566},
  number={7744},
  pages={484--488},
  year={2019},
  publisher={Nature Publishing Group}
}

@inproceedings{guidotti2018survey,
  title={A survey of methods for explaining black box models},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  booktitle={Proceedings of the 2018 International Conference on Data Mining Workshops (ICDMW)},
  pages={550--557},
  year={2018},
  organization={IEEE}
}

@article{castelvecchi2016can,
  title={Can we open the black box of AI?},
  author={Castelvecchi, Davide},
  journal={Nature},
  volume={538},
  number={7623},
  pages={20--23},
  year={2016},
  publisher={Nature Publishing Group}
}